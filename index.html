<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->

  <title>Mehmet Saygin Seyfioglu</title>
  
  <script>
    function toggleDropdown() {
        const container = document.querySelector('.dropdown-container');
        container.classList.toggle('active');
    }
  </script>
  
  <style>
    /* Base styles for the container */
    .dropdown-container {
        position: relative;
        width: 100%;
        max-width: 600px; /* or any desired max-width */
        margin: 20px auto;
    }

    /* The hidden content */
    .dropdown-content {
        display: none;
        width: 100%;
        border-top: 1px solid #aaa;
    }

    /* The button with a downward arrow */
    .dropdown-button {
        display: inline-block;
        background-color: transparent;
        border: none;
        font-size: 24px;
        cursor: pointer;
        position: absolute;
        bottom: 0;
        left: 50%;
        transform: translateX(-50%);
    }

    /* Display content when container is active */
    .dropdown-container.active .dropdown-content {
        display: block;
    }
  </style>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mehmet Saygin Seyfioglu</name>
              </p>

              <p style="text-align:center">
                <a href="data/saygin_resume.pdf">CV</a> &nbsp|&nbsp
                <a href="https://scholar.google.com.tr/citations?user=65TuoYUAAAAJ&hl=en">Google Scholar</a> &nbsp|&nbsp
                <a href="https://github.com/aldraus">GitHub</a>&nbsp|&nbsp
                <a href="https://twitter.com/mehsaygin">Twitter</a>&nbsp|&nbsp
                <a href="https://www.linkedin.com/in/mehmet-sayg%C4%B1n-seyfio%C4%9Flu-883811106/">LinkedIn</a>&nbsp
              </p>

              <p>I am a 5th year PhD student at <a href="https://grail.cs.washington.edu/people/">Graphics and Imaging Laboratory</a> at the <a href="https://www.cs.washington.edu/">University of Washington, Seattle</a>, where I work with <a href="https://scholar.google.com/citations?user=nBwaXUsAAAAJ&hl=en">Linda Shapiro</a> and <a href="https://scholar.google.com.tr/citations?user=IcqahyAAAAAJ&hl=en">Ranjay Krishna</a>. My research has been supported by the <a href="https://us.fulbrightonline.org/"> Fulbright Fellowship (2019-2021), and the <a href="https://gibhs.psychiatry.uw.edu/"> Garvey Institute (2021-2022)</a>.
              </p>
              <p>
              Previously, I interned at Amazon during the summers of 2020, 2021, 2022, and 2023, where I worked on a range of projects, from self-supervised learning to diffusion models for virtual try-on settings. Prior to my PhD, I worked at a tech company in Turkey for 2 years as an Machine Learning Researcher working on various projects on vision and language.
              <p>  

              

               
</p>
              msaygin[at]cs[dot]washington[dot]edu
          </p>

              
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/saygin.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests revolve around self-supervised learning in medicine at the intersection of vision and language. I am also interested in generative diffusion models.

              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/quilt1m.png" alt="clean-usnob" width="220" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2306.11207">
                <papertitle>Quilt-1M: One Million Image-Text Pairs for Histopathology.</papertitle>
              </a>
              <br>
              <strong>Mehmet Saygin Seyfioglu*</strong>, <a href="https://arxiv.org/search/cs?searchtype=author&query=Seyfioglu%2C+M+S">
                 Wisdom O. Ikezogwo*</a>, <a href="https://arxiv.org/search/cs?searchtype=author&query=Shapiro%2C+L">
                Fatemeh Ghezloo*, Dylan Geva, Fatwir S. Mohammed, Pavan K. Anand, Ranjay Krishna and Linda Shapiro</a>
              <br>
              <em>Neurips 2023</em> June 2023.
                <p><strong>Summary:</strong>We introduce Quilt-1M, largest to date vision-language dataset created to aid representation learning in histopathology, utilizing various resources including YouTube. The dataset, assembled using a blend of handcrafted models and tools like large language models and automatic speech recognition, expands upon existing datasets by integrating additional data from sources like Twitter and research papers. A pre-trained CLIP model fine-tuned with Quilt-1M significantly outperforms state-of-the-art models for classifying histopathology images across 13 benchmarks spanning 8 sub-pathologies.</p>
            <div>
    <strong><a href="https://github.com/wisdomikezogwo/quilt1m" target="_blank">Code</a></strong>
</div>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dreampaint.png" alt="clean-usnob" width="220" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2305.01257">
                <papertitle>DreamPaint: Few-Shot Inpainting of E-Commerce Items for Virtual Try-On without 3D Modeling.</papertitle>
              </a>
              <br>
              <strong>Mehmet Saygin Seyfioglu</strong>,K. Bouyarmane, S. Kumar, A. Tavanaei and I. B. Tutar</a>
              <br>
              <em>PrePrint</em> May 2023.
                <p><strong>Summary:</strong>DreamPaint is a framework for inpainting of e-commerce products onto user-provided context images, using only 2D images from product catalogs and user context. It utilizes few-shot fine-tuning of pre-trained diffusion models, allowing for accurate placement of products in images, preserving both product and context characteristics. DreamPaint outperforms state of the art inpainting modules in both subjective human studies and quantitative metrics, showcasing its potential for virtual try-ons in e-commerce.</p>

            </td>
          </tr>	


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mmae2.png" alt="clean-usnob" width="220" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2209.01534">
                <papertitle>Multi-modal Masked Autoencoders Learn Compositional Histopathological Representations.</papertitle>
              </a>
              <br>
              <strong>Mehmet Saygin Seyfioglu*</strong>, W.O. Ikezogwo*</a>, and <a>Linda Shapiro</a>
              <br>
              <em>Extended abstract: Machine Learning for Health (ML4H), </em> Dec 2022.
              <p><strong>Summary:</strong>We propose the use of of Multi-modal Masked Autoencoders (MMAE) in histopathology for self-supervised learning (SSL) on whole slide images. MMAE, utilizing specific compositionality of Hematoxylin & Eosin stained WSIs, shows superior performance over other state-of-the-art SSL techniques and supervised baselines in an eight-class tissue phenotyping task.</p>
             <div>
    <strong><a href="https://github.com/wisdomikezogwo/MMAE_Pathology" target="_blank">Code</a></strong>
</div>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bar.png" alt="clean-usnob" width="220" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2207.04574">
                <papertitle>Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimerâ€™s Disease.</papertitle>
              </a>
              <br>
              <strong>Mehmet Saygin Seyfioglu</strong>, Z. Liu, P. Kamath, S. Gangolli, S. Wang, T. Grabowski, and
L. Shapiro.</a>
              <br>
              <em>International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland,</em> March 2022.
             <p><strong>Summary:</strong>We introduce a novel framework for Alzheimer's disease detection using brain MRIs. We utilize a data augmentation method called Brain-Aware Replacements (BAR) to generate synthetic samples. Then we trained a soft-label-capable supervised contrastive loss which aims to learn the relative similarity of representations. Through fine-tuning, the model pre-trained with this framework exhibits superior performance in the Alzheimer's disease detection task, outperforming other training approaches.</p>

            <div>
    <strong><a href="https://drive.google.com/file/d/1rq7X-CMKfhevvApn5WxcYynOkBcpndSs/view" target="_blank">Talk</a></strong>
</div>
            </td>
          </tr>




         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mlim.png" alt="clean-usnob" width="220" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2207.04574">
                <papertitle>MLIM: Vision-and-Language Model Pre-training with Masked Language and Image Modeling.</papertitle>
              </a>
              <br>
              <strong>Mehmet Saygin Seyfioglu*</strong>, T. Arici*, T. Neiman, Y. Xu, S. Tran, T. Cilimbi, B. Zeng, and I. Tutar.</a>
              <br>
              <em>Preprint</em> September 2021.
             <p><strong>Summary:</strong>We introduce Masked Language and Image Modeling for enhancing Vision-and-Language Pre-training by merging Masked Language Modeling (MLM) and reconstruction (Recon) losses. We also propose Modality Aware Masking (MAM) which aims to boost cross-modality interaction and separately gauge text and image reconstruction quality. By coupling MLM + Recon tasks with MAM, we exhibit a simplified VLP methodology, demonstrating improved performance on downstream tasks within a proprietary e-commerce multi-modal datasetâ€‹.</p>
             

            </td>
          </tr>



            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/leveraging.png" alt="clean-usnob" width="220" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2207.04574">
                <papertitle>Leveraging Unlabeled Data for Glioma Molecular Subtype and Survival Prediction.</papertitle>
              </a>
              <br>
              N. Nuechterlein, B. Li, <strong>Mehmet Saygin Seyfioglu</strong>, M. S. SeyfioÄŸlu, S. Mehta, P. J. Cimino, and L. Shapiro.</a>
              <br>
              <em>ICPR</em> May 2020.
             <p><strong>Summary:</strong>We address radio-genomic challenges in glioma subtype and survival prediction by utilizing multi-task learning (MTL) to leverage unlabeled MR data and combine it with genomic data. Our MTL model significantly surpasses comparable models trained only on labeled MR data for glioma subtype prediction tasks. We also demonstrate that our model's embeddings enhance survival predictions beyond using MR or somatic copy number alteration data alone. Our methodology showcases the potential of MTL in harnessing multimodal data for improved glioma characterization and survival prediction.</p>


            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
            <p>
                I have some publications from my earlier work. Please see my <a href="https://scholar.google.com.tr/citations?user=65TuoYUAAAAJ&hl=en">Google Scholar</a> page to see them.
            </p>
        </td>
    </tr>


    

        </tbody></table>



    
        

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gates_center.png" alt="cs188" width="160" height="160">
            </td>
            <td width="75%" valign="center">
              <a href="https://courses.cs.washington.edu/courses/cse576/23sp/">{TA} CSE 576:  Computer Vision, Spring 2023.</a>
              <p></p><a href="https://courses.cs.washington.edu/courses/cse473/23wi/">{TA} CSE 473:  Introduction to Artificial Intelligence, Winter/Fall 2023.</a>
              <br>
            </td>

          </tr>
        </tbody></table>

<!--         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Others</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/imbizo2020.png" alt="cs188" width="160" height="160">
            </td>
            <td width="75%" valign="center">
              <p> <strong>Summer Schools</strong> : <a href="https://imbizo.africa/"> IBRO-SIMONS Computational neuroscience summer school | 2020 | CAPETOWN</a> </p>
              <br>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                <a href="https://people.eecs.berkeley.edu/~barron/">Website credits: Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
